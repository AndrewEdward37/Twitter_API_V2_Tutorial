{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An extensive guide to collecting Tweets using the newly released Academic Research Twitter API package and Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![header-photo-twitter-api-v2](https://github.com/AndrewEdward37/Twitter_API_V2_Tutorial/blob/main/images/header-image.png?raw=true)\n",
    "Background photo credit: [NASA](https://unsplash.com/photos/Q1p7bh3SHj8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Pre-requisites to Start](#2.-Pre-requisites-to-Start)\n",
    "3. [Bearer Token](#3.-Bearer-Token)\n",
    "4. [Create Headers](#4.-Create-Headers)\n",
    "5. [Create URL](#5.-Create-URL)\n",
    "6. [Connect to Endpoint](#6.-Connect-to-Endpoint)\n",
    "7. [Putting it all together](#7.-Putting-it-all-Together)\n",
    "8. [Save Results to CSV](#8.-Save-Results-to-CSV)\n",
    "9. [Looping Through Requests](#9.-Looping-Through-Requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "At the end of 2020, Twitter introduced a new Twitter API built from the ground up. Twitter API v2 comes with more features and data you can pull and analyze, new endpoints and a lot of functionalities.\n",
    "\n",
    "With the introduction of that new API, Twitter also introduced a new powerful free product for academics: [The Academic Research product track](https://developer.twitter.com/en/solutions/academic-research).\n",
    "\n",
    "The track grants free access to full-archive search and other v2 endpoints, with a volume cap of 10,000,000 tweets per month!\n",
    "If you want to know if you qualify for the track or not, check this [link](https://developer.twitter.com/en/solutions/academic-research/application-info).\n",
    "\n",
    "I have recently worked on a data analysis research project at Carnegie Mellon University using the capabilites that this track offers, and the power it gives you as a researcher is mindblowing!\n",
    "\n",
    "Yet since v2 of the API is fairly new, less resources exist if you run into issues through the process of collecting data for your research.\n",
    "\n",
    "So, in this article, I will go through a step-by-step process from setting up, accessing endpoints, to saving tweets collected in CSV format to use for analysis in the future.\n",
    "\n",
    "This article will be using an endpoint that is available only for the Academic Research track ([Full-archive Search](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all) endpoint), but almost everything in this guide can be applied to any other endpoint that is available for all developer accounts.\n",
    "\n",
    "If you do not have access to the Full-archive Search endpoint, you can still use follow this tutorial using the ([Recent Search](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent) endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-requisites to Start\n",
    "\n",
    "First, we are going to be importing some essential libraries for this guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to send your first request to the Twitter API, you need to have a developer account. If you don't have one yet, you can apply for one [here](https://developer.twitter.com/en/apply-for-access)! (Don't worry its free and you just need to provide some information about the research you plan on pursuing)\n",
    "\n",
    "Got an approved developer account? Fantastic!\n",
    "\n",
    "All you need to do is create a project and connect an App through the developer portal and we are set to go!\n",
    "\n",
    "1. Go to the [developer portal dashboard](https://developer.twitter.com/en/portal/dashboard)\n",
    "2. Sign in with your developer account\n",
    "3. Create a new Project, give it a name, a use-case based on the goal you want to achieve, and a description.\n",
    "\n",
    "![New Project page](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/new-project.png?token=AHQLCWF4J4IOFAMWKOQJGJTAU26HC)\n",
    "\n",
    "4. Assuming this is your first time, choose ‘create a new App instead’ and give your App a name in order to create a new App.\n",
    "\n",
    "![New App last step](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/name-app.png)\n",
    "\n",
    "If everything is successful, you should be able to see this page containing your keys and tokens, we will use one of these to access the API.\n",
    "\n",
    "![keys and token](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/tokens-and-keys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bearer Token\n",
    "\n",
    "If you have reached this step, CONGRATULATIONS! You are elligible send your first request from the API :))\n",
    "\n",
    "First we will create an auth() function that will have the \"Bearer Token\" from the app we just created.\n",
    "\n",
    "Since this Bearer Token is sensitive information, you should not be sharing it with anyone at all. If you are working with a team you don't want anyone to have access to it.\n",
    "\n",
    "So, we will save the token in an \"environment variable\".\n",
    "\n",
    "There are many ways to do this, you can check out these two options:\n",
    "\n",
    "- [Environment Variables .env with Python](https://www.youtube.com/watch?v=ecshCQU6X2U)\n",
    "- [Making Use of Environment Variables in Python](https://www.nylas.com/blog/making-use-of-environment-variables-in-python/)\n",
    "\n",
    "In this article, what we will do is we will just run these two lines in our code to set a \"TOKEN\" variable:\n",
    "\n",
    "```\n",
    "    import os\n",
    "    os.environ['TOKEN'] = '<ADD_BEARER_TOKEN>'\n",
    "```\n",
    "Just replace the <ADD_BEARER_TOKEN> with your bearer token and after you run the function, delete the two lines.\n",
    "\n",
    "If you get an error from this approach, try any of the links listed above.\n",
    "\n",
    "Now, we will create our auth() function, which retrieves the token from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return os.getenv('TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Headers\n",
    "\n",
    "Next we will define a function that will take our bearer token, pass it for authorization and return headers we will use to access the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create URL\n",
    "\n",
    "Now that we can access the API, we will build the request for the endpoint we are going to use and the parameters we want to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defined function above, contains two pieces:\n",
    "\n",
    "### A. *search_url:* Which is the link of the endpoint we want to access.\n",
    "\n",
    "Twitter's API has a lot of different endpoints, here is the list of endpoints currently available at the time of writing this article with early access:\n",
    "\n",
    "![endpoints](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/early-access-endpoints.png)\n",
    "\n",
    "You can also find the full list and more information about each of the endpoints in this [link](https://developer.twitter.com/en/docs/twitter-api/early-access).\n",
    "\n",
    "For this article, since it is targeted towards Academic Researchers who are possibly trying to benefit from Twitter's new product, we will be using the **full-archive search endpoint**.\n",
    "\n",
    "\n",
    "### B. *query_params:* The parameters that the endpoint offers and we can use to customize the request we want to send.\n",
    "\n",
    "Each endpoint has different parameters that we can pass to it, and of course Twitter has an API-reference for each of them in its documentation!\n",
    "\n",
    "For example for the full-archive search endpoint that we are using for this article, you can find the list of query parameters here in its [API Reference page](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all) under the \"Query parameters\" section.\n",
    "\n",
    "![Query parameters screenshot](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/query_parameters.png)\n",
    "\n",
    "We can decompose the query parameters above to three sections:\n",
    "\n",
    "![parameters-breakdown](https://github.com/AndrewEdward37/Twitter_API_V2_Tutorial/blob/main/images/parameters-breakdown.png?raw=true)\n",
    "\n",
    "1. The first 4 parameters are ones we are controlling\n",
    "\n",
    "```\n",
    "'query':        keyword,\n",
    "'start_time':   start_date,\n",
    "'end_time':     end_date,\n",
    "'max_results':  max_results,\n",
    "```\n",
    "\n",
    "2. The next 4 parameters are basically us instructing the endpoint to return more information that is optional that it won't return by default. \n",
    "\n",
    "```\n",
    "'expansions':   'author_id,in_reply_to_user_id,geo.place_id',\n",
    "'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "'user.fields':  'id,name,username,created_at,description,public_metrics,verified',\n",
    "'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "```\n",
    "\n",
    "3. Lastly, the 'next_token' parameter is used to get the next 'page' of results. The value used in the parameter is pulled directly from the response provided by the API if more results than the cap per request exist, we will talk more about this as we go in this article.\n",
    "\n",
    "```\n",
    "'next_token': {}\n",
    "```\n",
    "\n",
    "**NOTE:** It is important to note that since some of these are optional parameters, they might not exist for all tweets.\n",
    "\n",
    "For example, in 'expansions', the 'geo.place_id' will only return a result if a location is attached to the tweet retrieved. This will be important to remember when we save the results later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the create_url function does, a couple of important notes:\n",
    "\n",
    "- *Required endpoints:*\n",
    "\n",
    "    In the case of the full-archive search endpoint, the *query* parameter is the only parameter that is **required** to make a request. Always make sure to look at the documentation for the endpoint you are using to confirm which parameters HAVE to exist so you do not face issues.\n",
    "\n",
    "\n",
    "- *Query Parameter:*\n",
    "\n",
    "    The *query* parameter is where you put the keyword(s) you want to search for. Queries can be as simple as searching for tweets containing the word \"xbox\" or as complex as \"(xbox europe) OR (xbox usa)\" which will return tweets that contain the words xbox AND europe or xbox AND usa.\n",
    "    \n",
    "    Also, a *query* can be customized using *search operators*. There are so many options that help you narrow your search results. We will hopefully discuss operators more in depth in another article. For now, you can find the full list of operators for building queries [here](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
    "    \n",
    "    Example of a simple query with an operator: \"xbox lang:en\"\n",
    "    \n",
    "\n",
    "- *Timestamps:* \n",
    "\n",
    "    The *end_time* and *start_time* format that Twitter uses for timestamps is *YYYY-MM-DDTHH:mm:ssZ (ISO 8601/RFC 3339)*. So make sure to convert the dates you want to this format. If you are unsure about how to, this is a nice [timestamp converter](https://www.timestamp-converter.com/) that will definitely help.\n",
    "    \n",
    "\n",
    "- *Results volume:* \n",
    "\n",
    "    The number of search results returned by a request is currently limited between 10 and 500 results. \n",
    "\n",
    "Now you might be asking, how can I get more than 500 results then? **That is where *next_token* and pagination comes to play!**\n",
    "\n",
    "The answer is simple: If more results exists for your query, Twitter will return a unique *next_token* that you can use in your next request and it will give you the new results.\n",
    "\n",
    "If you want to retrieve all the tweets that exist for your query, you just keep sending requests using the new *next_token* you receive every time, until no *next_token* exists, signaling that you have retrieved all the tweets!\n",
    "\n",
    "\n",
    "Hopefully you are not feeling too confused! But don't worry, when we run all of the functions we just created, it will be clear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Connect to Endpoint\n",
    "\n",
    "Now that we have the url, headers, and parameters we want, we will create a function that will put all of this together and connect to the endpoint.\n",
    "\n",
    "The function below will send the \"GET\" request and if everything is correct (response code 200), it will return the response in \"json\" format.\n",
    "\n",
    "**Note:** *next_token* is set to \"None\" by default since we only care about it if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting it all Together\n",
    "\n",
    "Now that we have all the functions we need, lets test putting them all together to create our first request!\n",
    "\n",
    "In the next cell, we will setup our inputs:\n",
    "\n",
    "- bearer_token and headers from the API.\n",
    "- We will look for tweets in English that contain the word \"xbox\".\n",
    "- We will look for tweets between the 1st and the 31st of March, 2021.\n",
    "- We want only a maximum of 10 tweets returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for the request\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"xbox lang:en\"\n",
    "start_time = \"2021-03-01T00:00:00.000Z\"\n",
    "end_time = \"2021-03-31T00:00:00.000Z\"\n",
    "max_results = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the url and get the response from the API.\n",
    "\n",
    "The response returned from the Twitter API is returned in JavaScript Object Notation \"JSON\" format.\n",
    "To be able to deal with it and break-down the response we get, we will the encoder and decoder that exists for python which we have imported earlier. You can find more information about the library [here](https://docs.python.org/3/library/json.html).\n",
    "\n",
    "If the code of the returned response is *200*, then the request was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = create_url(keyword, start_time,end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json_response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets break-down the returned JSON response, the response is basically read as a Python dictionary and the keys either contain data or contain more dictionaries. The top two keys are:\n",
    "\n",
    "### A. Data:\n",
    "\n",
    "A list of dictionaries, each dictionary represents the data for a tweet.\n",
    "\n",
    "Example on how to retrieve the time from the first tweet was created:\n",
    "\n",
    "```\n",
    "    json_response['data'][0]['created_at']\n",
    "```\n",
    "\n",
    "### B. Meta:\n",
    "\n",
    "A dictionary of attributes about the request we sent, we usually would only care about two keys in this dictionary, *next_token* and *result_count*.\n",
    "\n",
    "![meta-data-response-breakdown](https://github.com/AndrewEdward37/Twitter_API_V2_Tutorial/blob/main/images/meta-data-response-breakdown.png?raw=true)\n",
    "\n",
    "Example on how to retrieve the next_token:\n",
    "\n",
    "```\n",
    "    json_response['meta']['result_count']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two options to save results depending on how we want to deal with the data, either we can save the results in the same JSON format we received, or in CSV format.\n",
    "To save results in JSON, we can easily do it using these two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as f:\n",
    "    json.dump(json_response, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Save Results to CSV\n",
    "\n",
    "You might be asking yourself, why do we want to save the results in CSV format? The short answer is, compared to a JSON object, CSVs are a widely used format that can easily be imported into an Excel spreadsheet, database, or data visualization software.\n",
    "\n",
    "\n",
    "Now, to save the results in a table format for CSV, there are two approaches, a simple approach and a more custom approach.\n",
    "\n",
    "Well...\n",
    "\n",
    "**If there is an simple approach through a Python library, why do we need the custom approach?**\n",
    "\n",
    "The answer to that is: The custom function will make us break-down and streamline the embedded dictionaries in some of the returned results into separate columns, making our analysis task easier.\n",
    "\n",
    "For example, the public metrics key:\n",
    "```\n",
    "\"public_metrics\": {\n",
    "                \"like_count\": 0,\n",
    "                \"quote_count\": 0,\n",
    "                \"reply_count\": 0,\n",
    "                \"retweet_count\": 0\n",
    "            },\n",
    "```\n",
    "The key returns another dictionary, the simple approach will save this how dictionary under one CSV column, while in the custom approach, we can break each into a different column before saving to the data to CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. The simple approach:\n",
    "\n",
    "For this approach we will be using the Pandas package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(response['json_response'])\n",
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was taken from [this blogpost](https://developer.twitter.com/en/docs/tutorials/five-ways-to-convert-a-json-object-to-csv) from the Twitter Dev team on this topic, which I tried and it works really well with simple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. The custom approach:\n",
    "\n",
    "First, we will create a CSV file with our desired column headers, we will do this separately from our actual function so later on it does not interfere with looping over requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file\n",
    "csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will create our *append_to_csv* function, that we will input the response and desired filename into, and the function will append all the data we collected to the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we run our append_to_csv() function on our last call, we shoul have a file that contains 10 tweets (or less depending on your query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets in API call:  10\n"
     ]
    }
   ],
   "source": [
    "append_to_csv(json_response, \"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Looping Through Requests\n",
    "\n",
    "Good job! We have sent our first request and saved the first response from it.\n",
    "\n",
    "Now, what if we want to save more responses? Beyond the first 500 results that Twitter gave us or if we want to automate getting Tweets over a specific period of time. For that we will be using loops and the next_token variables we receive from Twitter.\n",
    "\n",
    "Lets think about this case:\n",
    "\n",
    "We want to collect tweets that contained the word \"COVID-19\" in 2020 to analyse people's sentiment when tweeting about the virus. Probably millions of tweets exist, and we have a limit of collecting 10 Million tweets per month only.\n",
    "\n",
    "If we just send a request to collect tweets between the 1st of January 2020 and the 31st of December 2020, we will hit our cap very quickly without having a good distribution from all 12 months.\n",
    "\n",
    "So what we can do is, we can set a limit for tweets we want to collect per month, so that if we reach the specific cap at one month, we move on to the next one.\n",
    "\n",
    "The code below is an example that will just do that exactly! The block of code below is composed of two loops:\n",
    "\n",
    "1. For-loop that goes over the months/weeks/days we want to cover (Depending on how it is set)\n",
    "\n",
    "2. While-loop that controls the maximum number of tweets we want to collect per time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fosktkma06ysgm4wgynfrumz0sw5tp\n",
      "Start Date:  2021-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  490\n",
      "Total # of Tweets added:  490\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fosntcmiv0wh735qw6efvq5lhzawe5\n",
      "Start Date:  2021-02-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  487\n",
      "Total # of Tweets added:  977\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fosqtycrbq6s54n8yu7qrwyeeez6v1\n",
      "Start Date:  2021-03-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  494\n",
      "Total # of Tweets added:  1471\n",
      "-------------------\n",
      "Total number of results:  1471\n"
     ]
    }
   ],
   "source": [
    "#Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"xbox lang:en\"\n",
    "start_list =    ['2021-01-01T00:00:00.000Z',\n",
    "                 '2021-02-01T00:00:00.000Z',\n",
    "                 '2021-03-01T00:00:00.000Z']\n",
    "\n",
    "end_list =      ['2021-01-31T00:00:00.000Z',\n",
    "                 '2021-02-28T00:00:00.000Z',\n",
    "                 '2021-03-31T00:00:00.000Z']\n",
    "max_results = 500\n",
    "\n",
    "#Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file\n",
    "csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "csvFile.close()\n",
    "\n",
    "for i in range(0,len(start_list)):\n",
    "\n",
    "    # Inputs\n",
    "    count = 0 # Counting tweets per time period\n",
    "    max_count = 100 # Max tweets per time period\n",
    "    flag = True\n",
    "    next_token = None\n",
    "    \n",
    "    # Check if flag is true\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"data.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"data.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(5)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "In this article, we have gone through an extensive step-by-step process for collecting Tweets from Twitter API v2 for Academic Research using Python.\n",
    "\n",
    "We have covered the pre-requisites required, authentication, creating requests and sending requests to the search/all endpoint, and finally saving responses in different formats.\n",
    "\n",
    "If you liked this, feel free to share it with your friends and colleagues over Twitter and LinkedIn!\n",
    "\n",
    "Feel free to connect with me on LinkedIn or follow me on Twitter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit5904bd7f91754a36a971dd04ee67d6d0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
