{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# An Extensive Guide to collecting Tweets from Twitter API v2 for Academic Research using Python"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## List:\n",
    "\n",
    "1. Introduction\n",
    "2. Pre-requisites to start\n",
    "3. Bearer Token\n",
    "4. Create Headers\n",
    "5. Create URL\n",
    "6. Connect to Endpoint\n",
    "7. Save results to CSV\n",
    "8. Tweet collection, explained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Introduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "At the end of 2020, Twitter introduced a new Twitter API built from the ground up. Twitter API v2 comes with more features and data you can pull and analyze, new endpoints and a lot of functionalities.\n",
    "\n",
    "One of the most common reasons developers use the Twitter API is to listen to and analyze the conversation happening on Twitter.\n",
    "\n",
    "In this article, we will go through a step-by-step process from setting up, accessing endpoints, to saving tweets collected in CSV format to use for analysis in the future.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Pre-requisites\n",
    "\n",
    "First, we are going to be importing some essential libraries for this guide"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "\n",
    "\n",
    "# For file management when creating and adding to the dataset\n",
    "import os\n",
    "\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "source": [
    "To be able to send your first request to the Twitter API, you need to have a developer account. If you don't have one yet, you can apply for one [here](https://developer.twitter.com/en/apply-for-access)! (Don't worry its free and you just need to provide some information)\n",
    "\n",
    "Got an approved developer account? Fantastic!\n",
    "\n",
    "All you need to do is create a project and connect an App through the developer portal and we are set to go!\n",
    "\n",
    "1. Go to the [developer portal dashboard](https://developer.twitter.com/en/portal/dashboard)\n",
    "2. Sign in with your developer account\n",
    "3. Create a new Project, give it a name, a use-case based on the goal you want to achieve, and a description.\n",
    "\n",
    "![New Project page](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/new-project.png?token=AHQLCWF4J4IOFAMWKOQJGJTAU26HC)\n",
    "\n",
    "4. Assuming this is your first time, choose ‘create a new App instead’ and give your App a name in order to create a new App.\n",
    "\n",
    "![New App last step](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/name-app.png)\n",
    "\n",
    "If everything is successful, you should be able to see this page containing your keys and tokens, we will use one of these to access the API.\n",
    "\n",
    "![keys and token](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/tokens-and-keys.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Bearer Token\n",
    "\n",
    "If you have reached this step, CONGRATULATIONS! You can send your first request from the API :))\n",
    "\n",
    "First we will create an auth() function that will have the \"Bearer Token\" from the app we just created.\n",
    "\n",
    "Just replace the <ADD_BEARER_TOKEN> with your bearer token."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (((Remember to remove key before publishing)))\n",
    "def auth():\n",
    "    return 'AAAAAAAAAAAAAAAAAAAAAPOyOAEAAAAAvoDoQHKZfQWoXvLeMAxwyU0QQks%3Dj1pEstfU8e3iLZOQCULI4tipIp5lk0z16DnsHAOBwMcoCII2Q9'"
   ]
  },
  {
   "source": [
    "## 4. Create Headers\n",
    "\n",
    "Next we will define a function that will take our bearer token, pass it for authorization and return headers we will use to access the API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "source": [
    "## 5. Create URL\n",
    "\n",
    "Now that we can access the API, we will build the request for the endpoint we will use and the parameters we want to pass.\n",
    "The defined function below, contains two pieces:\n",
    "\n",
    "### A. *search_url:* Which is the link of the endpoint we want to access.\n",
    "\n",
    "Twitter's API has a lot of different endpoints, here is the list of endpoints currently available at the time of writing this article with early access:\n",
    "\n",
    "![endpoints](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/early-access-endpoints.png)\n",
    "\n",
    "You can also find the full list and more information about each of the [link](https://developer.twitter.com/en/docs/twitter-api/early-access).\n",
    "\n",
    "For this article, since it is targeted towards Academic Researchers who are possibly trying to benefit from Twitter's new product, we will be using the **full-archive search endpoint**.\n",
    "\n",
    "\n",
    "### B. *query_params:* The parameters that the endpoint offers and we can use to customize the request we want to send.\n",
    "\n",
    "Each endpoint has different parameters that we can pass to it, and of course Twitter has an API-reference for each of them!\n",
    "\n",
    "For example for the full-archive search endpoint that we are using for this article, you can find the list of Query parameters here in its [API Reference page](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all) under the \"Query parameters\" section.\n",
    "\n",
    "![Query parameters screenshot](https://raw.githubusercontent.com/AndrewEdward37/Twitter_API_V2_Tutorial/main/images/query_parameters.png)\n",
    "\n",
    "We can decompose the query below to three sections:\n",
    "\n",
    "1. The first 4 parameters are ones we are controlling\n",
    "\n",
    "```\n",
    "'query':        keyword,\n",
    "'start_time':   start_date,\n",
    "'end_time':     end_date,\n",
    "'max_results':  max_results,\n",
    "```\n",
    "\n",
    "2. The next 4 parameters are basically us instructing the endpoint to return more information that is optional that it won't return by default. \n",
    "\n",
    "```\n",
    "'expansions':   'author_id,in_reply_to_user_id,geo.place_id',\n",
    "'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "'user.fields':  'id,name,username,created_at,description,public_metrics,verified',\n",
    "'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "```\n",
    "\n",
    "3. Lastly, the 'next_token' parameter is used to get the next 'page' of results. The value used with the parameter is pulled directly from the response provided by the API, and should not be modified, we will talk more about this as we go in this article.\n",
    "\n",
    "```\n",
    "'next_token': {}\n",
    "```\n",
    "\n",
    "**NOTE:** It is important to note that since some of these are optional parameters, they might not exist for all tweets.\n",
    "\n",
    "For example, in 'expansions', the 'geo.place_id' will only return a result if a location is attached to the tweet retrieved. This will be important to remember when we save the results later on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "source": [
    "Now that we know what the create_url function does, a couple of important notes:\n",
    "\n",
    "- *Required endpoints:*\n",
    "\n",
    "    In the case of the full-archive search endpoint, the *query* parameter is the only parameter that is **required** to make a request. Always make sure to look at the documentation for the endpoint you are using to confirm which parameters HAVE to exist so you do not face issues.\n",
    "\n",
    "- *Query Parameter:*\n",
    "\n",
    "    The *query* parameter is where you put the keyword(s) you want to search for. Queries can be as simple as searching for tweets containing the word \"xbox\" or as complex as \"(xbox europe) OR (xbox usa)\" which will return tweets that contain the words xbox AND europe or xbox AND usa.\n",
    "    \n",
    "    Also, a *query* can be customized using *search operators*. There are so many options that help you narrow your search results. We will hopefully discuss operators more in depth in another article. For now, you can find the full list of operators for building queries [here](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
    "    \n",
    "    Example of a simple query with an operator: \"xbox lang:en\"\n",
    "\n",
    "- *Timestamps:* \n",
    "\n",
    "    The *end_time* and *start_time* format that Twitter uses for timestamps is *YYYY-MM-DDTHH:mm:ssZ (ISO 8601/RFC 3339)*. So make sure to convert the dates you want to this format. If you are unsure about how to, this is a nice [timestamp converter](https://www.timestamp-converter.com/) that will definitely help.\n",
    "\n",
    "- *Results volume:* \n",
    "\n",
    "    The number of search results returned by a request is currently limited between 10 and 500 results. \n",
    "\n",
    "Now you might be asking, how can I get more than 500 results then? **That is where *next_token* and pagination comes to play!**\n",
    "\n",
    "The answer is simple: If more results exists for your query, Twitter will return a unique *next_token* that you can use in your next request and it will give you the new results.\n",
    "\n",
    "If you want to retrieve all the tweets that exist for your query, you just keep sending requests using the new *next_token* you receive every time, until no *next_token* exists, signaling that you have retrieved all the tweets!\n",
    "\n",
    "\n",
    "Hopefully you are not feeling too confused! But don't worry, when we run all of the functions we just created, it will be clear!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6. Connect to Endpoint\n",
    "\n",
    "Now that we have the url, headers, and parameters we want, we will create a function that will put all of this together and connect to the endpoint.\n",
    "\n",
    "The function below will send the \"GET\" request and if everything is correct (response code 200), it will return the response in \"json\" format.\n",
    "\n",
    "**Note:** *next_token* is set to \"None\" by default since we only care about it if it exists."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "source": [
    "## 7. Save results to CSV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(json_response, fileName):\n",
    "\n",
    "    #Saving the tweets in a csv file:\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8-sig')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Counter to find how many tweets were retrieved\n",
    "    counter = 0\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # 1) Time \n",
    "        time_created = dateutil.parser.parse(tweet['created_at'])\n",
    "        \n",
    "        # 2) Tweet ID\n",
    "        tweet_id = tweet['id'].encode('utf-8')\n",
    "\n",
    "        # 3) Tweet text\n",
    "        # text = remove_url(tweet['text'])\n",
    "        text = tweet['text']\n",
    "        # print(text)    \n",
    "\n",
    "        # 4) Author ID\n",
    "        author_id = tweet['author_id'].encode('utf-8')\n",
    "\n",
    "        # 5) If A GEO location exist\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "        \n",
    "        # 6) Conversation ID\n",
    "        conversation_id = tweet['conversation_id'].encode('utf-8')\n",
    "\n",
    "        # 7) If \"tweet is a reply\" ID of the original tweet exist\n",
    "        if ('in_reply_to_user_id' in tweet):   \n",
    "            in_reply = tweet['in_reply_to_user_id'].encode('utf-8')\n",
    "        else:\n",
    "            in_reply = \" \"\n",
    "        \n",
    "        # 8) Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 9) Language of tweet\n",
    "        language = tweet['lang']\n",
    "\n",
    "        # Assemble all data in a list\n",
    "        res = [time_created, tweet_id, author_id, text, geo, language, conversation_id,in_reply,retweet_count,reply_count,like_count,quote_count]\n",
    "\n",
    "        # Look for context Annotations\n",
    "        if ('context_annotations' in tweet):\n",
    "            \n",
    "            # Only look at the first three\n",
    "            if len(tweet['context_annotations']) > 3:\n",
    "                min_len = 3\n",
    "            else:\n",
    "                min_len = len(tweet['context_annotations']) \n",
    "\n",
    "            #loop through the context content\n",
    "            for i in range(min_len):\n",
    "                domain_desc = tweet['context_annotations'][i]['domain']['description']\n",
    "                domain_id = tweet['context_annotations'][i]['domain']['id']\n",
    "                domain_name = tweet['context_annotations'][i]['domain']['name']\n",
    "                entity_id = tweet['context_annotations'][i]['entity']['id']\n",
    "                entity_name = unicodedata.normalize('NFKD', tweet['context_annotations'][i]['entity']['name']).encode('ascii','ignore')\n",
    "                \n",
    "                # Add the new context info to the row of data\n",
    "                res +=[domain_id, domain_name, domain_desc, entity_id, entity_name]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"Tweets in API call: \", counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}