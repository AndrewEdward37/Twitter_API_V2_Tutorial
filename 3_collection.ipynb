{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# An Extensive Guide to collecting Tweets from Twitter API v2 for Analysis using Python"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## List:\n",
    "\n",
    "1. Introduction\n",
    "2. Pre-requisites to start\n",
    "3. Bearer Token\n",
    "4. Create Headers\n",
    "5. Create URL\n",
    "6. Connect to Endpoint\n",
    "7. Save results to CSV\n",
    "8. Tweet collection, explained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Introduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 2. Pre-requisites"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\r\n",
    "import os\r\n",
    "import json\r\n",
    "import pandas as pd\r\n",
    "import csv\r\n",
    "import datetime\r\n",
    "import dateutil.parser\r\n",
    "import unicodedata\r\n",
    "import time\r\n",
    "import re\r\n",
    "\r\n",
    "#helper function used to remove the URL from a given string. This is helpful as the Tweets we get from the Twitter API often\r\n",
    "#contains the url of the Tweet, which could hinder the accuracy of our sentiment analysis tools.\r\n",
    "#Requires: a string\r\n",
    "#Ensures: The same txt string with url's removed.\r\n",
    "def remove_url(txt):\r\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "source": [
    "## 3. Bearer Token"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (((Remember to remove key before publishing)))\n",
    "def auth():\n",
    "    return 'AAAAAAAAAAAAAAAAAAAAAPOyOAEAAAAAvoDoQHKZfQWoXvLeMAxwyU0QQks%3Dj1pEstfU8e3iLZOQCULI4tipIp5lk0z16DnsHAOBwMcoCII2Q9'"
   ]
  },
  {
   "source": [
    "## 4. Create Headers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "source": [
    "## 5. Create URL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(keyword, start_date, end_date, max_results,):\n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': 500,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "source": [
    "## 6. Save results to CSV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(json_response, fileName):\n",
    "\n",
    "    #Saving the tweets in a csv file:\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8-sig')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Counter to find how many tweets were retrieved\n",
    "    counter = 0\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # 1) Time \n",
    "        time_created = dateutil.parser.parse(tweet['created_at'])\n",
    "        \n",
    "        # 2) Tweet ID\n",
    "        tweet_id = tweet['id'].encode('utf-8')\n",
    "\n",
    "        # 3) Tweet text\n",
    "        # text = remove_url(tweet['text'])\n",
    "        text = tweet['text']\n",
    "        # print(text)    \n",
    "\n",
    "        # 4) Author ID\n",
    "        author_id = tweet['author_id'].encode('utf-8')\n",
    "\n",
    "        # 5) If A GEO location exist\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "        \n",
    "        # 6) Conversation ID\n",
    "        conversation_id = tweet['conversation_id'].encode('utf-8')\n",
    "\n",
    "        # 7) If \"tweet is a reply\" ID of the original tweet exist\n",
    "        if ('in_reply_to_user_id' in tweet):   \n",
    "            in_reply = tweet['in_reply_to_user_id'].encode('utf-8')\n",
    "        else:\n",
    "            in_reply = \" \"\n",
    "        \n",
    "        # 8) Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 9) Language of tweet\n",
    "        language = tweet['lang']\n",
    "\n",
    "        # Assemble all data in a list\n",
    "        res = [time_created, tweet_id, author_id, text, geo, language, conversation_id,in_reply,retweet_count,reply_count,like_count,quote_count]\n",
    "\n",
    "        # Look for context Annotations\n",
    "        if ('context_annotations' in tweet):\n",
    "            \n",
    "            # Only look at the first three\n",
    "            if len(tweet['context_annotations']) > 3:\n",
    "                min_len = 3\n",
    "            else:\n",
    "                min_len = len(tweet['context_annotations']) \n",
    "\n",
    "            #loop through the context content\n",
    "            for i in range(min_len):\n",
    "                domain_desc = tweet['context_annotations'][i]['domain']['description']\n",
    "                domain_id = tweet['context_annotations'][i]['domain']['id']\n",
    "                domain_name = tweet['context_annotations'][i]['domain']['name']\n",
    "                entity_id = tweet['context_annotations'][i]['entity']['id']\n",
    "                entity_name = unicodedata.normalize('NFKD', tweet['context_annotations'][i]['entity']['name']).encode('ascii','ignore')\n",
    "                \n",
    "                # Add the new context info to the row of data\n",
    "                res +=[domain_id, domain_name, domain_desc, entity_id, entity_name]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"Tweets in API call: \", counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}